---
title: "Artificial Intelligence Regulation in the European Union"
subtitle: "An overview as per December 2021"
author: "Sigrid Keydana"
format:
  revealjs: 
    theme: moon
    slide-number: true
    self-contained: true
    preview-links: true
    footer: AI Impact Working Group @RStudio, PBC
---

## Agenda

<br />

1.  Privacy- and human rights-related legislation: Foundations

2.  Privacy- and human rights-related legislation: Drafts / Proposals (as per 12/2021)

3.  The Proposed Artificial Intelligence Act

# Privacy- and human rights-related legislation: Foundations and present

## European Convention on Human Rights (1953)

<br />

-   Issued by the Council of Europe (founded in 1949)

-   Legal institution: European Court of Human Rights (ECtHR, Strasbourg).

<br />

::: {style="font-size: 60%"}
**Article 8 -- Right to respect for private and family life**

*Everyone has the right to respect for his private and family life, his home and his correspondence.*

*There shall be no interference by a public authority with the exercise of this right except such as is in accordance with the law and is necessary in a democratic society in the interests of national security, public safety or the economic well-being of the country, for the prevention of disorder or crime, for the protection of health or morals, or for the protection of the rights and freedoms of others.*
:::

## Charter of Fundamental Rights of the European Union (2009)

-   Effective as of entry into force of Treaty of Lisbon (2009)

-   Enforced by: Court of Justice of the European Union (CJEU, Luxembourg)

::: {style="font-size: 58%"}
**Article 7 - Respect for private and family life**\
*Everyone has the right to respect for his or her private and family life, home and communications.*

**Article 8 - Protection of personal data**

1.  *Everyone has the right to the protection of personal data concerning him or her.*

2.  *Such data must be processed fairly for specified purposes and on the basis of the consent of the person concerned or some other legitimate basis laid down by law. Everyone has the right of access to data which has been collected concerning him or her, and the right to have it rectified.*

3.  *Compliance with these rules shall be subject to control by an independent authority.*
:::

## General Data Protection Regulation (GDPR, 2018)

-   applies whenever *personal* data are collected, used, or stored

-   rights:

    -   absolute: to be informed, to access, to rectification, to data portability

    -   restricted: to erasure, to restrict processing, to object

-   roles: controller, joint controller, processor

## GDPR & AI: Privacy Impact Assessment (PIA)

A *Privacy Impact Assessment* must always be conducted when the processing could result in a high risk to the rights and freedoms of natural persons. E.g.,

-   [scoring/profiling,]{style="font-size: 70%;"}

-   [automatic decisions which lead to legal consequences for those impacted,]{style="font-size: 70%;"}

-   [systematic monitoring,]{style="font-size: 70%;"}

-   [processing of *special* personal data,]{style="font-size: 70%;"}

-   [the merging or combining of data which was gathered by various processes,]{style="font-size: 70%;"}

-   [data transfer to countries outside the EU/EEC]{style="font-size: 70%;"}

# Privacy- and human rights-related legislation: Drafts / Proposals (as per 12/2021)

## e-Privacy Directive (2002, amended 2009)

-   [rules on data retention, cookies, e-mail communication (among others)]{style="font-size: 70%;"}

-   [will be repealed by ePrivacy *Regulation* once that's been adopted]{style="font-size: 70%;"}

-   [based on Article 16 and Article 114 of the *Treaty on the Functioning of the European Union (TFEU)*[^1], which lays out rules for the internal market. [^2]]{style="font-size: 70%;"}

[^1]: The second of the two main treaties of the European Union, a continuation of the original Treaty of Rome (1958). The other is the treaty of Lisbon (2009), replacing the Treaty of Maastricht (1993).

[^2]: as well as Article 7 of the Charter of Fundamental Rights

[Criticisms:]{style="font-size: 70%;"}

-   [broadly formulated exceptions to privacy of electronic communications data, including content, to "ensure network and end user device security"]{style="font-size: 70%;"}

-   [allows for further processing "for compatible purposes"]{style="font-size: 70%;"}

-   [overall, criticized as [step back compared to the GDPR](https://edpb.europa.eu/system/files/2021-03/edpb_statement_032021_eprivacy_regulation_en_0.pdf)]{style="font-size: 70%;"}

::: notes
Directive vs. regulation:

A regulation is immediately binding; each country has to adapt its laws to make them comply.

A directive is individually implemented into national laws by each country so as to be at least minimally compliant.
:::

## Data Governance Act (11/2020)

-   covers: re-use of public data; "data sharing services/intermediaries"; "data altruism"

-   where data includes *personal* data

-   *"*without prejudice to Regulation (EU) 2016/679 and Directive 2002/58/EC"

::: {style="font-size: 60%;"}
"*Indeed, considering that data protection is a fundamental right guaranteed by Article 8 of the Charter, and taking into account that one of the main purposes of the GDPR is to provide data subjects with control over personal data relating to them, the EDPB reiterates that personal data cannot be considered as a "tradeable commodity". An important consequence of this is that, even if the data subject can agree to the processing of his or her personal data, he or she cannot waive his or her fundamental rights."* [^3]
:::

[^3]: EDPB-EDPS Statement 05/2021 on the Data Governance Act in light of the legislative developments

## Digital Services Act (12/2020)

-   defines a *layered* set of responsibilities for intermediaries (network infrastructure), hosting services, online platforms, and "very large online platforms"

-   Concerns:[^4]

    -   allows for use of AI systems categorizing individuals from biometrics according to ethnicity, gender, as well as political or sexual orientation

    -   allows emotion recognition

    -   allows targeted advertising

[^4]: [https://edpb.europa.eu/system/files/2021-11/edpb_statement_on_the_digital_services_package_and_data_strategy_en.pdf]{style="font-size: 50%;"}

## Digital Markets Act (12/2020)

<br />

-   sets up ex-ante rules for so-called *gatekeepers*, including

    -   to refrain from combining personal data from different sources

    -   to submit (on an annual basis) an independently audited description of any techniques deployed for profiling consumers

# The Proposed Artificial Intelligence Act

## Objectives

<br />

::: {style="font-size: 80%;"}
The general objective is to '*ensure the proper functioning of the internal market by creating the conditions for the development and use of trustworthy artificial intelligence in the Union*' (impact assessment, p. 32).

The specific objectives are:

\(i\) to ensure that AI systems placed on the market and used are safe and respect *existing rules* on fundamental rights and Union values,

\(ii\) to ensure legal certainty to *facilitate investment and innovation* in AI,

\(iii\) to enhance governance and effective enforcement of *existing rules* on fundamental rights and safety requirements applicable to AI systems, and

\(iv\) to facilitate the *development of a single market* for lawful, safe and trustworthy AI applications and *prevent market fragmentation*. [^5]
:::

[^5]: [https://www.europarl.europa.eu/RegData/etudes/BRIE/2021/694212/EPRS_BRI(2021)694212_EN.pdf]{style="font-size: 50%;"}

## Approach

<br />

Risk-based:

-   Unacceptable risk: prohibited (absolutely or with exceptions)

-   High risk: catalog of requirements

-   Limited risk: transparency requirements

-   Minimal risk: "encourage" and "facilitate" voluntary codes of conduct

## "Ethics washing made in Europe" [^6]

[^6]: [https://www.tagesspiegel.de/politik/eu-guidelines-ethics-washing-made-in-europe/24195496.html]{style="font-size: 50%;"}

![](ethics_washing_2.png){.absolute top="228" left="50"}

![](ethics_washing_1.png){.absolute top="70" right="0"}

![](ethics_washing_1.png){.absolute top="80" right="0"}

![](ethics_washing_3.png){.absolute bottom="0" right="200"}

## Unacceptable risk (1, prohibited): Manipulation (1)

<br />

::: {style="font-size: 80%;"}
The placing on the market, putting into service or use of an AI system that deploys subliminal techniques beyond a person's consciousness [in order to]{style="color: red;"} materially distort a person's behaviour in a manner that causes or is likely to cause that person or another person physical or psychological [harm]{style="color:red;"}
:::

<br />

::: {style="font-size: 80%;"}
*"\[a\]n inaudible sound \[played\] in truck drivers' cabins to push them to drive longer than healthy and safe \[where\] AI is used to find the frequency maximising this effect on drivers"*
:::

<br />

## Unacceptable risk (1, prohibited): Manipulation (2)

<br />

::: {style="font-size: 80%;"}
The placing on the market, putting into service or use of an AI system that exploits any of the vulnerabilities of a specific group of persons due to their age, physical or mental disability, [in order to]{style="color:red"} materially distort the behaviour of a person pertaining to that group in a manner that causes or is likely to cause that person or another person physical or psychological [harm]{style="color:red"}
:::

<br />

::: {style="font-size: 70%;"}
*"\[a\] doll with integrated voice assistant \[which\] encourages a minor to engage in progressively dangerous behavior or challenges in the guise of a fun or cool game"*
:::

## Unacceptable risk (1, prohibited): Problems [^7]

[^7]: [Essentially following [Veale & Borgesius, Demystifying the Draft EU Artificial Intelligence Act](https://arxiv.org/abs/2107.03721).]{style="font-size: 50%;"}

<br >

-   [Harm]{style="color:red"}: does not consider cumulative harm

-   [Intent]{style="color:red"}:

    -   [hard to prove]{style="font-size: 70%;"}

    -   [does not consider *dual use*]{style="font-size: 70%;"}

    -   [does not consider the usual newspeak / marketing strategies]{style="font-size: 70%;"}

    -   [leaves out dynamics in the user base]{style="font-size: 70%;"}

-   Does not add much to existing EU law (Unfair Commercial Practices Directive)

## Unacceptable risk (2, prohibited): Social scoring

<br />

::: {style="font-size: 80%;"}
The sale or use of AI systems [used by or on behalf of public authorities]{style="color: red;"}, to generate trustworthiness scores and which lead to either unjustified or disproportionate treatment of individuals or groups, or detrimental treatment which, while justifiable and proportionate, occurs in an [unrelated context]{style="color: red;"} from the input data
:::

**Problems:** [^8]

[^8]: [Essentially following [Veale & Borgesius, Demystifying the Draft EU Artificial Intelligence Act](https://arxiv.org/abs/2107.03721).]{style="font-size: 50%;"}

-   What is an [unrelated context]{style="color: red;"} is up to definition

-   [Public authorities]{style="color: red;"}: What about, e.g., privately controlled delivery, telecommunications or transport?

## Unacceptable risk (3, partially prohibited): Biometric systems

Some [uses]{style="color: red;"} of [real-time]{style="color: red;"} biometric systems in publicly accessible spaces [by law enforcement]{style="color: red;"}

**Problems**: [^9]

[^9]: [Essentially following [Veale & Borgesius, Demystifying the Draft EU Artificial Intelligence Act](https://arxiv.org/abs/2107.03721).]{style="font-size: 50%;"}

::: {style="font-size: 70%;"}
-   [Usage]{style="color: red;"}, not selling

<!-- -->

-   Only if by [law enforcement]{style="color: red;"}

-   [Real-time]{style="color: red;"} only

-   [Exceptions]{style="color: red;"}: search for victims/missing children; threat to life or physical safety / terrorist attack; detection, localisation, identification or prosecution of a perpetrator or suspect of a crime with maximum sentence of at least 3 years
:::

## High risk: To health, safety and fundamental rights

<br />

... in a *number of defined applications, products and sectors*.

<br />

Based on and entwined with the New Legislative Framework (NLF) (the New Approach when introduced in 1985), a common EU approach to the regulation of certain products such as lifts, medical devices, personal protective equipment and toys

## High risk: Applicability

1.  [AI systems that are products or safety components (broadly construed) of products already covered by certain Union health and safety harmonisation legislation (such as toys, machinery, lifts, or medical devices).]{style="font-size: 70%;"}
2.  [Standalone AI systems specified in an annex for use in eight fixed areas:]{style="font-size: 70%;"}

::: {style="font-size: 80%;"}
    1.  biometric identification and categorisation (both remote and offline);

    2.  management and operation of critical infrastructure;

    3.  educational and vocational training;

    4.  employment, worker management and access to self-employment;

    5.  access to and enjoyment of essential services and benefits;

    6.  law enforcement;

    7.  migration, asylum and border management;

    8.  administration of justice and democracy.
:::

## High risk: Requirements

<br />

**What:**

::: {style="font-size: 80%;"}
Risk management system; data quality criteria; accuracy; robustness and cybersecurity; technical documentation; logging; human oversight.
:::

**Who:**

::: {style="font-size: 80%;"}
Providers, not users.
:::

**How:**

::: {style="font-size: 80%;"}
Certification, *or* follow standards developed by 3 European Standardisation Organisations (ESOs).
:::

## High-risk: Problems [^10]

[^10]: [Essentially following [Veale & Borgesius, Demystifying the Draft EU Artificial Intelligence Act](https://arxiv.org/abs/2107.03721).]{style="font-size: 50%;"}

-   ::: {style="font-size: 80%;"}
    -   Can add sub-areas within these areas, if similar risk to an existing in-scope application, but [cannot add new areas]{style="color:red;"} entirely.

    -   Datasets only need to meet requirements "sufficiently" and "in view of the intended purpose of the system"

    -   No explicit discussion of leakage of training data or other personal data from models

    -   Unclear if [essential stakeholders]{style="color:red;"} will be involved in the standardisation process

    -   Certification bodies are [private]{style="color:red;"} bodies!

    -   Restrictions are on providers, however [the provider may not know how system is used]{style="color:red;"}.
    :::

## Limited risk: To health, safety and fundamental rights

For "limited-risk" applications, either provider or user have to fulfill *transparency* requirements.

Three categories are named:

1.  *Bot* disclosure. Responsible: Provider.
2.  *Emotion recognition* and *biometric categorisation* disclosure. Responsible: Provider.
3.  *Synthetic content* ("Deep Fake") disclosure. Responsible: User.

## Limited risk: Problems [^11]

[^11]: [See [Stark & Hoey, The Ethics of Emotion in Artificial Intelligence Systems](https://dl.acm.org/doi/10.1145/3442188.3445939).]{style="font-size: 50%;"}

-   Provider vs. user boundary not that sharp; each alone may lack information
-   Emotion recognition is anything but limited-risk.
    -   [Scientific models of emotion are under-complex, and data reflect this]{style="font-size: 70%; color: red;"}

    -   [Emotions, and their expression, are inseparable from social, cultural, situational context]{style="font-size: 70%; color: red;"}

    -   [Emotions are intimately related to human dignity]{style="font-size: 70%; color: red;"}

    -   [Emotions are often subject to moral judgement]{style="font-size: 70%; color: red;"}

    -   [In consequence, social credit effects arise]{style="font-size: 70%; color: red;"}

## General Problems: Legal / technical

-   Risk-based approach is based on context; but in practice, models/applications are ported to [different contexts.]{style="color: red;"}

-   [Does]{style="Concrete lists of applications, combined with selective group of people/lobbyists involved, means that: - solution is solution as defined by dominant party, not people impacted by stuff or representing minorities"} [not reflect important ways AI is being used]{style="color: red;"}, for example: No covering of Artificial Intelligence as a Service (AIaaS)[^12].

-   Does not apply to systems already [in use]{style="color: red;"}.

-   [Excludes]{style="color: red;"} international law enforcement cooperation.

-   [Preemptive effect]{style="color: red;"}: Aims to '"prevent unilateral Member States actions that risk to fragment the market and to impose even higher regulatory burdens \[...\]'".

[^12]: [See: https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3824736).]{style="font-size: 50;"}

::: notes
**AIaaS**

-   Providers claim not to be controllers, but they are as soon as they do "supplementary processing" (e.g., use customer data to refine models) they are; namely

    -   joint controllers for data input and analysis stages

    -   single controller for supplementary processing

    -   Worse, all that is really third-party data - would need to obtain specific, informed consent!

    -   But the provider can't, and the customer does not know about the specifics of supplementary processing, and has to obtain a generic opt-in in general

    -   Thus, providers are likely in strong violation of GDPR

-   The dominant-subordinate understanding of controller-processor relationships, still depicted in GDPR, does not reflect the actual power dynamics in many contemporary environments.

-   AIaaS leads to an amplification / spreading of ethical problems with AI:

    -   Issues are likely to propagate across many different applications operating in many different areas

    -   Many issues will manifest only in particular contexts, the scope for which providers are unlikely to have considered. This is known as the "portability trap".

-   Overall, a vicious circle of monopolization results.
:::

## General Problems: Society & Human Rights

-   Concrete lists of applications, combined with the massive lobbying that has been going on, means that this is the [rule of a dominant party]{style="color: red;"}, not people impacted / affected, or minorities.

-   Leaves out risks for [groups of individuals]{style="color: red;"} or the [society]{style="color: red;"} as a whole.

-   No reference to the [individual affected]{style="color: red;"} / no regard to [human rights]{style="color: red;"}.

-   Not enough focus on [data protection]{style="color: red;"}.

# References

## Legal (AI Act)

::: {style="font-size: 60%;"}
[Proposal for a Regulation Of The European Parliament And Of The Council Laying Down Harmonised Rules On Artificial Intelligence (Artificial Intelligence Act) And Amending Certain Union Legislative Acts](https://eur-lex.europa.eu/legal-content/EN-FR-IT/TXT/?from=EN&uri=CELEX%3A52021PC0206).

[Veale, M., Borgesius, F.Z.: Demystifying the Draft EU Artificial Intelligence Act](https://arxiv.org/abs/2107.03721).

[EDPB-EDPS Joint Opinion 5/2021 on the proposal for a Regulation of the European Parliament and of the Council laying down harmonised rules on artificial intelligence (Artificial Intelligence Act)](https://edpb.europa.eu/system/files/2021-06/edpb-edps_joint_opinion_ai_regulation_en.pdf)

[Initial Appraisal of a European Commission Impact Assessment, Artificial Intelligence Act](https://www.europarl.europa.eu/RegData/etudes/BRIE/2021/694212/EPRS_BRI(2021)694212_EN.pdf)

[Ethics washing made in Europe](https://www.tagesspiegel.de/politik/eu-guidelines-ethics-washing-made-in-europe/24195496.html)
:::

## Legal (Other)

::: {style="font-size: 60%;"}
[EDPB-EDPS Statement 03/2021 on the ePrivacy Regulation](https://edpb.europa.eu/system/files/2021-03/edpb_statement_032021_eprivacy_regulation_en_0.pdf)

[https://www.cr-online.de/blog/2021/09/19/in-der-datenschutzrechtlichen-todeszone/](https://www.cr-online.de/blog/2021/10/07/in-der-datenschutzrechtlichen-todeszone-der-data-governance-act-teil-i/).

[BfDI kritisiert Position des Rats zur ePrivacy-Verordnung](https://www.bfdi.bund.de/SharedDocs/Pressemitteilungen/DE/2021/03_Ratsposition-ePrivacy-VO.html)

[EDPB-EDPS Statement 05/2021 on the Data Governance Act in light of the legislative developments](https://edpb.europa.eu/system/files/2021-05/edpb_statementondga_19052021_en_0.pdf)

[EDPB-EDPS Statement on the Digital Services Package and Data Strategy](https://edpb.europa.eu/system/files/2021-11/edpb_statement_on_the_digital_services_package_and_data_strategy_en.pdf)

[EDPB-EDPS Opinion 2/2021 on the Proposal for a Digital Markets Act](https://edps.europa.eu/system/files/2021-02/21-02-10-opinion_on_digital_markets_act_en.pdf)

[Algorithm Watch Joint Statement on the Ad Hoc Committee on Artificial Intelligence (CAHAI) in the Council of Europe](https://algorithmwatch.org/en/joint-statement-cahai/)
:::

## Fairness, ethics, etc.

::: {style="font-size: 60%;"}
[Selbst, A. et al.: Fairness and Abstraction in Sociotechnical Systems](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3265913).

[Stark, L., Hoey, J.: The Ethics of Emotion in Artificial Intelligence Systems](https://dl.acm.org/doi/10.1145/3442188.3445939).

[EDRI, Beyond Debiasing](https://edri.org/wp-content/uploads/2021/09/EDRi_Beyond-Debiasing-Report_Online.pdf.)

[Cobbe, J., Singh, J.: Artificial Intelligence as a Service: Legal Responsibilities, Liabilities, and Policy Challenges](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3824736).
:::
